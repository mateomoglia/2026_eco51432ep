[
  {
    "objectID": "part3_viz.html",
    "href": "part3_viz.html",
    "title": "Data visualisation",
    "section": "",
    "text": "In this part, we are going to learn how to:\n\nMake appealing data visualization with ggplot2 package",
    "crumbs": [
      "Data visualization"
    ]
  },
  {
    "objectID": "part3_viz.html#roadmap",
    "href": "part3_viz.html#roadmap",
    "title": "Data visualisation",
    "section": "",
    "text": "In this part, we are going to learn how to:\n\nMake appealing data visualization with ggplot2 package",
    "crumbs": [
      "Data visualization"
    ]
  },
  {
    "objectID": "part3_viz.html#plots-with-ggplot2",
    "href": "part3_viz.html#plots-with-ggplot2",
    "title": "Data visualisation",
    "section": "Plots with ggplot2",
    "text": "Plots with ggplot2\nWe are going, by practicing over several plots, to make -hopefully- appealing data visualization. We are going to take advantage of the/temp/idf_data.csv we built earlier on., monu_histo.csv containing the list of all monuments listed in Ile-de-France with the zip code, gares.shp listing all stations in the Île-de-France train network, with their associated location.\n\n      path = \"C:/Users/mateomoglia/Dropbox/courses/polytechnique/2026_eco51432ep\"\n      \n      library(dplyr) # Load the dplyr package\n      library(ggplot2) # Make graphs\n      library(tidyr) # To manipulate data\n      library(plotly) # Interactive graphs\n\n\nAttachement du package : 'plotly'\n\n\nL'objet suivant est masqué depuis 'package:ggplot2':\n\n    last_plot\n\n\nL'objet suivant est masqué depuis 'package:stats':\n\n    filter\n\n\nL'objet suivant est masqué depuis 'package:graphics':\n\n    layout\n\n      library(sf) # For spatial data\n\nLinking to GEOS 3.12.2, GDAL 3.9.3, PROJ 9.4.1; sf_use_s2() is TRUE\n\n      # Open the data ----------------------------------------------------------\n      # Data were created before and saved under /temp/idf_data.csv\n      \n      idf_data = read.csv2(paste0(path,\"/temp/idf_data.csv\")) %&gt;%\n        mutate(log_loy_appartement = log(loy_appartement),\n               log_loy_house       = log(loy_house),\n               log_popmun = log(popmun))\n\n\nGeneral description\nThis package is a workhorse of data visualization in R. The structure is somewhat similar to the dplyr approach. The idea is to apply to a ggplot() object a series of functions to add elements to the plot and control the appearance of the latter. The syntax requires:\n\nData, from which the graph will be made\nMapping, a set of instructions to “structure” the appearance of the graphs. Usually, it sets the axes, the grouping variables, among other things. It is also referred as aesthetics\nLayers, which converts the data and the mapping into geometries (points, lines, regression lines, bar charts, etc.)\n[opt.] Scales, to control the appearance of the mapping (axes length, colors, etc.)\n[opt.] Guides/Themes, to control the general appearance of the graph\n\nUsually, the ggplot function takes two main arguments and one to many sub-arguments. The two main arguments are (i) the data on which the visualization is required and (ii) an aesthetic. The aesthetic controls the key element of the visualization, such as the x and y axes or the grouping colors/linetypes.\nLet’s imagine we have a dataset df with x_val and y_val values and a grouping variable (gender, country, etc.). Hence, to plot a scatter plot, we would code:\n\nggplot(df, mapping = aes(x = x_val, y = y_val, colour = gender)) + # Set the graph\n    geom_point() + # Add points\n    theme_void() # Control the general appearance of the graph\n\n\n\nTo export\nThe standard way to save plot created with ggplot() is to use the function ggsave(). It takes several arguments:\n\nplot: by default, it saves the latest plot (opened in the Plots pane) or the ggplot object created\npath: the path where to save the plot. Do not forget the extension! For instance, .pdf, .jpg, etc.\ndpi: the quality of the plot (I use dpi=300 as a trade-off between lisibility and size)\nwidth and height: the width and the height of the created object\nunit: the unit for width and height (inches, pixels, centimeters)",
    "crumbs": [
      "Data visualization"
    ]
  },
  {
    "objectID": "part1_data.html",
    "href": "part1_data.html",
    "title": "Open data",
    "section": "",
    "text": "In this section, we are going to learn how to:\n\nOpen datasets\nPerform simple data cleaning tasks",
    "crumbs": [
      "Open data"
    ]
  },
  {
    "objectID": "part1_data.html#roadmap",
    "href": "part1_data.html#roadmap",
    "title": "Open data",
    "section": "",
    "text": "In this section, we are going to learn how to:\n\nOpen datasets\nPerform simple data cleaning tasks",
    "crumbs": [
      "Open data"
    ]
  },
  {
    "objectID": "part1_data.html#open-datasets",
    "href": "part1_data.html#open-datasets",
    "title": "Open data",
    "section": "Open datasets",
    "text": "Open datasets\n\nData format\nAs you may know, data come in various formats. Here are some common formats:\n\n\n\nExtension\nFormat\nDescription\nEfficiency ⭐\n\n\n\n\n.csv\nCSV\nComma-separated values\n⭐⭐\n\n\n.tsv\nTSV\nTab-separated values\n⭐⭐\n\n\n.xlsx\nXLSX\nExcel spreadsheet\n⭐\n\n\n.RData\nRData\nR’s format\n⭐⭐⭐\n\n\n.json\nJSON\nJavaScript\n⭐⭐\n\n\n.parquet\nPARQUET\nColumnar storage\n⭐⭐⭐⭐⭐\n\n\n.sql\nSQL\nStructured database storage\n⭐⭐⭐⭐\n\n\n\nMany other formats exist, depending on the source or the type of data stored (eg, spatial data).\n\n\nOpen data in R\nWe are going to manipulate mostly .csv database. There are non-proprietary and can be opened by virtually all softwares. This is not the most efficient way to store data. If you manipulate very large database (tens of millions of observations with hundreds of columns), I advise you to use .parquet.\nBase R includes a function to open .csv files with the function read.csv2(). Some packages might be required to open other formats. To open large .csv dataset, I advise you to use the fread() function from the package data.table.\nThe syntax is:\n\ndata = read.csv2(path = \"C:/your/path/data_to_open.csv\")\n\n\n\nWrite data\nOnce you are done cleaning the data, you might want to save your dataset (this is actually good practice!). Most functions to read data, have a writing function counterpart. Hence:\n\nwrite.csv2(data, path = \"C:/your/path/name_of_the_dataset.csv\")\n\n\n\nUse a paste()\nYou might want to save a bit of time, reduce the probability of writing errors, and increase the reliability and reproducibility of your code by using a path object within a paste() function. This function, from base R, simply concatenates two strings. For instance paste(\"Hello\",\"world\",sep = \" \") will produce \"Hello world\". An alternative function is paste0() which, by default, concatenates without space. Hence, paste(\"Hello\",\"world\",sep = \"\") and paste0(\"Hello\",\"world\") would be stricly equivalent and prints Helloword.\nIt comes handy with the read.csv2() and the write.csv2() functions. Instead of rewriting the total length each time, you can call the path object you created earlier on. If the dataset idf_data.csv is stored in the subfolder /data, it is easy to open it:\n\npath = \"/Users/mateomoglia/Dropbox/courses/polytechnique/2026_eco51432ep\"\n\ndata = read.csv2(path = paste0(path,\"/data/idf_data.csv\"))\n\nIt is useful if you share code with someone. You need to change only the line with the path, instead of the whole code.\n\n\nDiscover the data\nI prepared a dataset, mainly drawn from INSEE data, and downloaded from the Institut Paris Region, a research organization on urbanism in the Ile-de-France region. It contains various socio-economic variables at the municipality (and Paris’ arrondissement) level. It contains information on demographic characteristics, income, and on housing market variables.\nThe codebook contains all variables and a quick description.",
    "crumbs": [
      "Open data"
    ]
  },
  {
    "objectID": "part1_data.html#data-cleaning",
    "href": "part1_data.html#data-cleaning",
    "title": "Open data",
    "section": "Data cleaning",
    "text": "Data cleaning\nData cleaning refers to going from a raw dataset to a dataset than can be used and manipulated efortlessly. Below, here are some examples of data cleaning procedures:\n\nFormat column names (eg, replace spaces with underscores with function rename());\nRemove missing values if they are real missing values with mutate() and ifelse();\nReplace weird values by NAs or 0s (eg, most INSEE missing wage values are 99999 and should be refered as NAs, and not as 99999) with mutate() and ifelse();\nClean/create ID variables (for instance, municipal ID in France is made of the département code in 2-digit and the municipality in 3-digit. To create a municipal ID, one needs to concatenate both) with mutate() and paste0();\nPut variables in the right format (eg, numeric variables such as wage should be… numeric and not string – super frequent with .csv datasets that do not store the column format) with mutate() and as.numeric() or as.character();\nOverall visual inspection (check if no weird values, use the summary() or head() functions)\n\nAlthough time consuming, this step is particularly important to (i) familiarize with the data and (ii) save precious time when it comes to the data analysis.\nUsually, data come from a codebook, a dictionnary, or some kind of documentation (another dataset or a .pdf). It is super useful to read them carefully to understand how data are reported (missing values, column names, variable formatting) .\nFor the purpose of the class, data are already cleaned but never take data as face value!",
    "crumbs": [
      "Open data"
    ]
  },
  {
    "objectID": "part1_data.html#data-manipulation",
    "href": "part1_data.html#data-manipulation",
    "title": "Open data",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nMerge datasets\nIn real world situation, data come from many sources and/or are delivered in many datasets (one per year, or per area, for instance). Then, you want to merge them to obtain a bigger and more insightful datasets.\n\n\n\n\n\n\nNote\n\n\n\nBecause R is an objet-based language, you can manipulate several datasets within the same function. However, this is perilous as you have to make sure that they have the same dimensions, are formated the same, etc. Merging reduces the probability to face those issues\n\n\nIn dplyr, the merging is done through the joint functions. A join between a df1 and a df2 has to be done on at least one joining variable. There are several types of joints:\n\n\n\n\n\n\n\nJoin Type\nDescription\n\n\n\n\ninner_join()\nReturns rows with matching keys in both tables\n\n\nleft_join()\nReturns all rows from the left table, with matches from the right if present\n\n\nright_join()\nReturns all rows from the right table, with matches from the left if present\n\n\nfull_join()\nReturns all rows from both tables, matching where possible\n\n\nsemi_join()\nReturns rows from the left table that have a match in the right table\n\n\nanti_join()\nReturns rows from the left table that do not have a match in the right table\n\n\n\nHence, the code should look like that:\n\nmerged_df = left_join(df1, df2)\n\nIf they do not share a common variable, an approach is to use the option by = c(df1$x, df2$y):\n\nmerged_df = left_join(df1, df2, by = c(df1$x,df2$y))\n\nOf course, this is compatible with the pipe syntax:\n\nmerged_df = df1 %&gt;%\n  left_join(df2)\n\n\n\nReshape datasets\nData manipulation also goes with reshaping the datasets. Let’s say a dataset contains the population at the municipal level. The first column is the municipality ID and each other column in the population for a given year, such that: codgeo, pop1999, pop2000, until pop2023.\nTo obtain a panel dataset (multiple IDs observed multiple times), we need to reshape the datasets in long format (as opposed to wide). The function is pivot_longer(). This function is from the library tidyr. You need to download it first and load it.\n\ndata_long = data_wide %&gt;%\n  tidyr::pivot_longer(\n    cols = starts_with(\"pop\"), # Might have used -codgeo\n    names_to = \"year\",         # New col name\n    values_to = \"population\",  # New col name\n    names_prefix = \"pop\"       # Remove the prefix \n  ) %&gt;%\n  mutate(year = as.integer(year)) # Not necessary, but usually year is numeric\n\nThe counterpart function is tidyr::pivot_wider().\n\n\nOther manipulations\nIn dplyr, it is possible to manipulate data using the following functions:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelects rows based on logical conditions (eg, keep Ile de France)\n\n\nselect()\nChooses columns by name\n\n\narrange()\nReorders rows based on column values\n\n\ndistinct()\nReturns unique rows\n\n\nslice()\nSelects rows by position or helper functions (e.g., slice_head())\n\n\nbind_rows()\nStack two dataframes by rows\n\n\nbind_cols()\nCombine two dataframes side by side\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nOpen the population dataset /raw/idf_pop.csv and check if is clean\nCreate a subset containing only codgeo and the columns that start with popmun\nMake it long with codgeo as the ID and a year column\nExtract the most recent year (ideally, using a function)\nOpen the housing cost dataset raw/idf_rents.csv\nCreate a dataset with 3 columns: codgeo, loy_sqm_appartment and loy_sqm_house\nMerge the population datasets (population in the most recent year and the population by CSP) and the housing cost datasets on municipality ID\nSave the newly merged dataset in .csv in an appropriate subfolder.\n\nThis latter dataset will be our main dataset for the rest of the class.",
    "crumbs": [
      "Open data"
    ]
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Welcome to this class!",
    "section": "",
    "text": "This website will walk you through the tutorials for the Introduction to R course of Ecole Polytechnique’s Master program. The course has 3 main objectives: (i) know the basics of R (ii) manipulate the standard statistical and empirical tools (iii) make appealing output.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "home.html#class-outline",
    "href": "home.html#class-outline",
    "title": "Welcome to this class!",
    "section": "Class outline",
    "text": "Class outline\n\nPart 0: First hand on R\n\nDiscover R and its IDE, RStudio\ntidyverse environment and packages\n\nPart 1: Open data\n\nOpen and manipulate data\n\nPart 2: Descriptive statistics\n\nCompute descriptive statistics\nExport them in .tex\nLinear regression\n\nPart 3: Data visualization\n\nScatter plots\nBar charts\nLinear regression\nDensity and box plots\n\nPart 4: Spatial data manipulation\n\nShapefiles\nCompute distances\nMaps\n\n\nDuring sessions, I will walk you through all materials covered here. I provide some exercises that you are required to perform.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "home.html#about-this-site-and-my-inspirations",
    "href": "home.html#about-this-site-and-my-inspirations",
    "title": "Welcome to this class!",
    "section": "About this site and my inspirations",
    "text": "About this site and my inspirations\nI created this site for teaching purpose. It is of course perfectible and any constructive comment is more than welcome (write me an email!). I took inspiration from these sources:\n\nThe great Florian Oswald teaching material at Sciences Po. Do not hesitate to check his teaching material (in English)\nThe tutorials of the French Ministry of Ecology (in French)\nThe infamous online book Econometrics with R (in English)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/part0_R.html",
    "href": "docs/part0_R.html",
    "title": "First hands on R",
    "section": "",
    "text": "For this first part, we are going to\n\nDownload R and Rstudio\nDiscover the software\nDig into the tidyverse environment"
  },
  {
    "objectID": "docs/part0_R.html#roadmap",
    "href": "docs/part0_R.html#roadmap",
    "title": "First hands on R",
    "section": "",
    "text": "For this first part, we are going to\n\nDownload R and Rstudio\nDiscover the software\nDig into the tidyverse environment"
  },
  {
    "objectID": "docs/part0_R.html#what-is-r-set-up-and-launch",
    "href": "docs/part0_R.html#what-is-r-set-up-and-launch",
    "title": "First hands on R",
    "section": "What is R? Set-up and launch",
    "text": "What is R? Set-up and launch\nR is a free and open-source software used in many contexts: data cleaning, data visualisation, econometrics, machine and deep learning, among others. It is supported by a lively and active user community. R is an object-based language, meaning that we are going to manipulate objects through a series of commands.\nR can be used directly from a command terminal, but we prefer to use RStudio, an IDE, to make our lifes easier. An IDE is an interface that eases the use of a software. The most popular one is RStudio. To use it, we need to:\n\nDownload R\nThen, download RStudio\n\n\nYou can download both from here https://posit.co/download/rstudio-desktop/."
  },
  {
    "objectID": "docs/part0_R.html#general-best-practices",
    "href": "docs/part0_R.html#general-best-practices",
    "title": "First hands on R",
    "section": "General best practices",
    "text": "General best practices\nBefore jumping to coding per se, let me recall you some best practices when performing computer-based tasks. Knowing those practices is relevant for several reasons:\n\nSave yourself time in the future: it might me bothersome at first, but the future you will thank the current you\nResearch reproducibility is a high-stake aspect of research\nIT is probably more pleasant to work in a tidy and clean environment than in a messy one\n\n\nFolder organization\nComputers are organized around folders. We are going to work in a working directory (wd), but we need to identify it. My folder tree looks like it:\n/Users/mmoglia/Dropbox/\n├── Documents/\n│   ├── perso/\n│   │   ├── banque/\n│   │   ├── admin/\n│   │   └── festival/\n├── Downloads/\n├── Music/\n├── Pictures/\n│   ├── famille/\n│   ├── vacances/\n└── courses/\n│   ├── polytechnique/\n|   |   ├── 2024_eco102/\n|   |   ├── 2025_eco1s002/\n|   |   ├── 2026_eco51423ep/\n├── research/\nHere we are going to work in ~/courses/2026_eco51423ep/. This folder may (and should!) contains subfolders, for instance: /code, /output, /raw_data, etc. Each time I start a project, I always create those ones – with a similar naming conventions in all my projects. I stick to these rules to save time when navigating between projects (and when changing computers).\n\n\nNaming convention\nA typical tip is to choose simple and short titles for the files and the scripts. For instance, this file is named part0_r.qmd. Your code can be named code_tutorial.R. It should be self-explanatory.\n\n\n\n\n\n\nTip\n\n\n\nAvoid at all cost to use spaces or special characters in your file names. Prefer instead an underscore.\n\n\n\n\nWhen writing code\n\nAlways comment your code, make it readable for your future self but also for others. You may use your code in a week, a month, a year, and should be able to directly understands it! Comments in R starts with #.\nWhen creating objects, functions, or whatever, give it simple but understandable names.\nFor instance, if you create a data frame containing wages for individuals aged between 25 and 30, you way call it wage_25_30 and not w2530 (too short) or wage_individuals_aged_25_30 (too long).\nMoreover, especially for large project, you want to have different scripts. Always keep scripts short and with clear names. For instance 1_clean_data.R, 2_import_wages.R, 3_merge_datasets.R, and 4_data_analysis.R."
  },
  {
    "objectID": "docs/part0_R.html#first-step-in-rrstudio",
    "href": "docs/part0_R.html#first-step-in-rrstudio",
    "title": "First hands on R",
    "section": "First step in R/Rstudio",
    "text": "First step in R/Rstudio\n\nSome useful terms\n\nSo far, we talked about script.\nWe are going to manipulate objects using functions.\nObjects can be of different classes: matrix, vector, character, numeric, data.frame, etc. Some functions are already in R but some are not. Hence, we need to load them thanks to packages.\n\n\n\nOpen a script\n\nWe are going to create our first script (the extension is .R).\nThis script contains all commands you want to run. R has built-in functions but most useful functions should be called using library().\nThese libraries should be first downloaded then loaded into your project.\nFirst, open RStudio and opens a new script.\nYou should have an empty page. You may want to write some words at the top of it to have an idea of what this code does. You can use # to comment code.\n\n\n\nSeek for help and documentation\nTo know more about a function, simply type ?function in the console. Of course, internet is your friend, and ChatGPT (or any other LLMs) are pretty good at R. As I learnt how to use R in the LLM-era, I, of course, prefer to use Stack or other online forums instead!\n\n\nScript\n\n#-------------------------------------------------------------------------------\n# This script opens a dataset and proceeds\n# to data visualization.\n#\n# Author: Matéo Moglia\n# Date: 12/02/2025\n#-------------------------------------------------------------------------------\n\n# Set your working directory\nsetwd(\"C:/Users/mateomoglia/Dropbox/courses/polytechnique/2026_eco51432ep\")\npath &lt;- \"/Users/mateomoglia/Dropbox/courses/polytechnique/2026_eco51432ep\"\n\nHere, I set the working directory using setwd(). Notice that we created our first object, named path using the assignment operator &lt;- (I personally prefer to use = which works exactly the same). This object will turn out to be useful later on.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the class of path? Hint: use the function class()\n\n\n\n\nOur first data manipulation\nWe can load a very famous built-in data set, the iris dataset, into an object called data.\n\ndata = iris # Attribute to data the data set iris\nclass(data) # Check its class\n\n[1] \"data.frame\"\n\nhead(data) # Preview the first lines of data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nNotice that the two objects we created so far can be seen in the Environment pane.\n\n\nData description\nBy clicking on the object in the Environement panel, you can visualize it, but you can also write some code to describe it. As we just check, data is a data.frame object. It is made of several columns and rows.\nTo call a specific column in a dataframe, we use the $ operator. To know all the column names in data, we can use the function names(). To know the “size” of the dataframe, the right function is dim(): the first element is the number of rows and the second the number of columns.\n\n\n\n\n\n\nTip\n\n\n\nYou can also call a column by its position in the dataframe. For instance, to call the second column you can type data[,2]. For the second row: data[,2]. What should you write to print the 10th element of the 3rd column?\n\n\nTo describe the data, we can use several built-in functions.\n\ntable(data$Species) # To make a frequency table\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\nsummary(data$Sepal.Length) # To provide summary statistics\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.300   5.100   5.800   5.843   6.400   7.900 \n\n\nNow, we want to visualize the data. To do so, we can use base R function plot(). It takes at least one argument, on the x axis. Here, with two arguments, it plots a simple scatter plot.\n\nplot(iris$Sepal.Length,iris$Sepal.Width)"
  },
  {
    "objectID": "docs/part0_R.html#the-tidyverse",
    "href": "docs/part0_R.html#the-tidyverse",
    "title": "First hands on R",
    "section": "The tidyverse",
    "text": "The tidyverse\n\nWhy and how use it?\nThere are several ways to code in R. The first one, as we just saw, is called “base R”. Although useful to perform very simple task, it becomes inefficient for more difficult tasks. The most popular approach in the R community is by far the tidyverse approach. The idea is simple: we are going to apply a series of functions on an object to modify it or create a new object.\nIts popularity arises from two important factors: it is simple and straightforward to understand and it is faster than base R. A third approach is data.table. It is the fastest approach but requires a high entry cost and is not straightforward.\nTo use it, we can download the package dplyr and load it into our session. You need to download the package once (as long as you keep the same computer) but to load it each time you open RStudio. This is true for all packages. Note that all the function from the base package are natively loaded into R.\n\ninstall.packages(\"dplyr\") # Notice the quotation marks\nlibrary(dplyr) # Notice the absence of quotation marks\n\n\n\n\n\n\n\nTip\n\n\n\nMost of time, a function exists only in one package. However, it occurs that two packages load functions with the same name. To make sure you use the function from the desired package, you can use the package::function() syntax. In my research, I usually have to deal with that with the function select() that is loaded in dplyr and in sf (a package for spatial data manipulation). Hence, if both packages are loaded and I want to use the one from dplyr, I have to write dplyr::select().\n\n\n\n\nGeneral philosophy\nEach function are going to be separated by a pipe operator %&gt;%. The idea is as following. We apply on the object data several functions in order to create a new data object, new_data. These functions, for instance, create a new column, slice the data, drop the duplicates, remove the missing values, group the data by group, change the type of column (for numeric to character), compute summary statistics, etc.\n\nnew_data = data %&gt;%\n    function1(col) %&gt;%\n    function2(col) %&gt;%\n    function3(col)\n\nBelow, I list the most common dplyr functions.\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelect rows that meet certain conditions\n\n\nselect()\nPick specific columns from a data frame\n\n\narrange()\nReorder rows by column values\n\n\nmutate()\nCreate or modify columns\n\n\nsummarise()\nCollapse data into summary statistics\n\n\ngroup_by()\nDefine groups for grouped operations\n\n\njoin()\nCombine datasets by matching keys\n\n\ndistinct()\nKeep only unique rows\n\n\nrename()\nChange column names\n\n\nslice()\nSelect rows by position\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUsing the data dataframe, create a sub-dataset new_data such that:\n\nIt contains only the species setosa\nThe column Sepal.Length is now named sepal_length\nIt contains a new column large_width which takes 1 if the sepal width is bigger than 3 (hint: use an ifelse(cond,yes,false) function)\n\n\n\nNow, we want to simply print the number of observations for each value of large_width. To do so:\n\nnew_data %&gt;%\n    group_by(large_width) %&gt;% \n    summarize(count = n())\n\n# A tibble: 2 × 2\n  large_width count\n        &lt;dbl&gt; &lt;int&gt;\n1           0     2\n2           1    48\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can try with other summarizing function. For instance: summarize(mean = mean(Sepal.Width))."
  },
  {
    "objectID": "part0_r.html",
    "href": "part0_r.html",
    "title": "First hands on R",
    "section": "",
    "text": "For this first part, we are going to\n\nDownload R and Rstudio\nDiscover the software\nDig into the tidyverse environment",
    "crumbs": [
      "First hand on R"
    ]
  },
  {
    "objectID": "part0_r.html#roadmap",
    "href": "part0_r.html#roadmap",
    "title": "First hands on R",
    "section": "",
    "text": "For this first part, we are going to\n\nDownload R and Rstudio\nDiscover the software\nDig into the tidyverse environment",
    "crumbs": [
      "First hand on R"
    ]
  },
  {
    "objectID": "part0_r.html#what-is-r-set-up-and-launch",
    "href": "part0_r.html#what-is-r-set-up-and-launch",
    "title": "First hands on R",
    "section": "What is R? Set-up and launch",
    "text": "What is R? Set-up and launch\nR is a free and open-source software used in many contexts: data cleaning, data visualisation, econometrics, machine and deep learning, among others. It is supported by a lively and active user community. R is an object-based language, meaning that we are going to manipulate objects through a series of commands.\nR can be used directly from a command terminal, but we prefer to use RStudio, an IDE, to make our lifes easier. An IDE is an interface that eases the use of a software. The most popular one is RStudio. To use it, we need to:\n\nDownload R\nThen, download RStudio\n\n\nYou can download both from here https://posit.co/download/rstudio-desktop/.",
    "crumbs": [
      "First hand on R"
    ]
  },
  {
    "objectID": "part0_r.html#general-best-practices",
    "href": "part0_r.html#general-best-practices",
    "title": "First hands on R",
    "section": "General best practices",
    "text": "General best practices\nBefore jumping to coding per se, let me recall you some best practices when performing computer-based tasks. Knowing those practices is relevant for several reasons:\n\nSave yourself time in the future: it might me bothersome at first, but the future you will thank the current you\nResearch reproducibility is a high-stake aspect of research\nIT is probably more pleasant to work in a tidy and clean environment than in a messy one\n\n\nFolder organization\nComputers are organized around folders. We are going to work in a working directory (wd), but we need to identify it. My folder tree looks like it:\n/Users/mmoglia/Dropbox/\n├── Documents/\n│   ├── perso/\n│   │   ├── banque/\n│   │   ├── admin/\n│   │   └── festival/\n├── Downloads/\n├── Music/\n├── Pictures/\n│   ├── famille/\n│   ├── vacances/\n└── courses/\n│   ├── polytechnique/\n|   |   ├── 2024_eco102/\n|   |   ├── 2025_eco1s002/\n|   |   ├── 2026_eco51423ep/\n├── research/\nHere we are going to work in ~/courses/2026_eco51423ep/. This folder may (and should!) contains subfolders, for instance: /code, /output, /raw_data, etc. Each time I start a project, I always create those ones – with a similar naming conventions in all my projects. I stick to these rules to save time when navigating between projects (and when changing computers).\n\n\nNaming convention\nA typical tip is to choose simple and short titles for the files and the scripts. For instance, this file is named part0_r.qmd. Your code can be named code_tutorial.R. It should be self-explanatory.\n\n\n\n\n\n\nTip\n\n\n\nAvoid at all cost to use spaces or special characters in your file names. Prefer instead an underscore.\n\n\n\n\nWhen writing code\n\nAlways comment your code, make it readable for your future self but also for others. You may use your code in a week, a month, a year, and should be able to directly understands it! Comments in R starts with #.\nWhen creating objects, functions, or whatever, give it simple but understandable names.\nFor instance, if you create a data frame containing wages for individuals aged between 25 and 30, you way call it wage_25_30 and not w2530 (too short) or wage_individuals_aged_25_30 (too long).\nMoreover, especially for large project, you want to have different scripts. Always keep scripts short and with clear names. For instance 1_clean_data.R, 2_import_wages.R, 3_merge_datasets.R, and 4_data_analysis.R.",
    "crumbs": [
      "First hand on R"
    ]
  },
  {
    "objectID": "part0_r.html#first-step-in-rrstudio",
    "href": "part0_r.html#first-step-in-rrstudio",
    "title": "First hands on R",
    "section": "First step in R/Rstudio",
    "text": "First step in R/Rstudio\n\nSome useful terms\n\nSo far, we talked about script.\nWe are going to manipulate objects using functions.\nObjects can be of different classes: matrix, vector, character, numeric, data.frame, etc. Some functions are already in R but some are not. Hence, we need to load them thanks to packages.\n\n\n\nOpen a script\n\nWe are going to create our first script (the extension is .R).\nThis script contains all commands you want to run. R has built-in functions but most useful functions should be called using library().\nThese libraries should be first downloaded then loaded into your project.\nFirst, open RStudio and opens a new script.\nYou should have an empty page. You may want to write some words at the top of it to have an idea of what this code does. You can use # to comment code.\n\n\n\nSeek for help and documentation\nTo know more about a function, simply type ?function in the console. Of course, internet is your friend, and ChatGPT (or any other LLMs) are pretty good at R. As I learnt how to use R in the LLM-era, I, of course, prefer to use Stack or other online forums instead!\n\n\nScript\n\n#-------------------------------------------------------------------------------\n# This script opens a dataset and proceeds\n# to data visualization.\n#\n# Author: Matéo Moglia\n# Date: 12/02/2025\n#-------------------------------------------------------------------------------\n\n# Set your working directory\nsetwd(\"C:/Users/mateomoglia/Dropbox/courses/polytechnique/2026_eco51432ep\")\npath &lt;- \"/Users/mateomoglia/Dropbox/courses/polytechnique/2026_eco51432ep\"\n\nHere, I set the working directory using setwd(). Notice that we created our first object, named path using the assignment operator &lt;- (I personally prefer to use = which works exactly the same). This object will turn out to be useful later on.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the class of path? Hint: use the function class()\n\n\n\n\nOur first data manipulation\nWe can load a very famous built-in data set, the iris dataset, into an object called data.\n\ndata = iris # Attribute to data the data set iris\nclass(data) # Check its class\n\n[1] \"data.frame\"\n\nhead(data) # Preview the first lines of data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nNotice that the two objects we created so far can be seen in the Environment pane.\n\n\nData description\nBy clicking on the object in the Environement panel, you can visualize it, but you can also write some code to describe it. As we just check, data is a data.frame object. It is made of several columns and rows.\nTo call a specific column in a dataframe, we use the $ operator. To know all the column names in data, we can use the function names(). To know the “size” of the dataframe, the right function is dim(): the first element is the number of rows and the second the number of columns.\n\n\n\n\n\n\nTip\n\n\n\nYou can also call a column by its position in the dataframe. For instance, to call the second column you can type data[,2]. For the second row: data[,2]. What should you write to print the 10th element of the 3rd column?\n\n\nTo describe the data, we can use several built-in functions.\n\ntable(data$Species) # To make a frequency table\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\nsummary(data$Sepal.Length) # To provide summary statistics\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.300   5.100   5.800   5.843   6.400   7.900 \n\n\nNow, we want to visualize the data. To do so, we can use base R function plot(). It takes at least one argument, on the x axis. Here, with two arguments, it plots a simple scatter plot.\n\nplot(iris$Sepal.Length,iris$Sepal.Width)",
    "crumbs": [
      "First hand on R"
    ]
  },
  {
    "objectID": "part0_r.html#the-tidyverse",
    "href": "part0_r.html#the-tidyverse",
    "title": "First hands on R",
    "section": "The tidyverse",
    "text": "The tidyverse\n\nWhy and how use it?\nThere are several ways to code in R. The first one, as we just saw, is called “base R”. Although useful to perform very simple task, it becomes inefficient for more difficult tasks. The most popular approach in the R community is by far the tidyverse approach. The idea is simple: we are going to apply a series of functions on an object to modify it or create a new object.\nIts popularity arises from two important factors: it is simple and straightforward to understand and it is faster than base R. A third approach is data.table. It is the fastest approach but requires a high entry cost and is not straightforward.\nTo use it, we can download the package dplyr and load it into our session. You need to download the package once (as long as you keep the same computer) but to load it each time you open RStudio. This is true for all packages. Note that all the function from the base package are natively loaded into R.\n\ninstall.packages(\"dplyr\") # Notice the quotation marks\nlibrary(dplyr) # Notice the absence of quotation marks\n\n\n\n\n\n\n\nTip\n\n\n\nMost of time, a function exists only in one package. However, it occurs that two packages load functions with the same name. To make sure you use the function from the desired package, you can use the package::function() syntax. In my research, I usually have to deal with that with the function select() that is loaded in dplyr and in sf (a package for spatial data manipulation). Hence, if both packages are loaded and I want to use the one from dplyr, I have to write dplyr::select().\n\n\n\n\nGeneral philosophy\nEach function are going to be separated by a pipe operator %&gt;%. The idea is as following. We apply on the object data several functions in order to create a new data object, new_data. These functions, for instance, create a new column, slice the data, drop the duplicates, remove the missing values, group the data by group, change the type of column (for numeric to character), compute summary statistics, etc.\n\nnew_data = data %&gt;%\n    function1(col) %&gt;%\n    function2(col) %&gt;%\n    function3(col)\n\nBelow, I list the most common dplyr functions.\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelect rows that meet certain conditions\n\n\nselect()\nPick specific columns from a data frame\n\n\narrange()\nReorder rows by column values\n\n\nmutate()\nCreate or modify columns\n\n\nsummarise()\nCollapse data into summary statistics\n\n\ngroup_by()\nDefine groups for grouped operations\n\n\njoin()\nCombine datasets by matching keys\n\n\ndistinct()\nKeep only unique rows\n\n\nrename()\nChange column names\n\n\nslice()\nSelect rows by position\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUsing the data dataframe, create a sub-dataset new_data such that:\n\nIt contains only the species setosa\nThe column Sepal.Length is now named sepal_length\nIt contains a new column large_width which takes 1 if the sepal width is bigger than 3 (hint: use an ifelse(cond,yes,false) function)\n\n\n\nNow, we want to simply print the number of observations for each value of large_width. To do so:\n\nnew_data %&gt;%\n    group_by(large_width) %&gt;% \n    summarize(count = n())\n\n# A tibble: 2 × 2\n  large_width count\n        &lt;dbl&gt; &lt;int&gt;\n1           0     2\n2           1    48\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can try with other summarizing function. For instance: summarize(mean = mean(Sepal.Width)).",
    "crumbs": [
      "First hand on R"
    ]
  },
  {
    "objectID": "part2_desc.html",
    "href": "part2_desc.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "In this part, we are going to learn how to:\n\nCompute summary statistics\nExport them into .tex file\nRun simple linear regressions\n\nSo far, please check that you:\n\nOpened RStudio, wrote your preambule, created a path object\nLoad the data idf_data.csv into RStudio\nLoad the package dplyr",
    "crumbs": [
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "part2_desc.html#roadmap",
    "href": "part2_desc.html#roadmap",
    "title": "Descriptive statistics",
    "section": "",
    "text": "In this part, we are going to learn how to:\n\nCompute summary statistics\nExport them into .tex file\nRun simple linear regressions\n\nSo far, please check that you:\n\nOpened RStudio, wrote your preambule, created a path object\nLoad the data idf_data.csv into RStudio\nLoad the package dplyr",
    "crumbs": [
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "part2_desc.html#descriptive-statistics",
    "href": "part2_desc.html#descriptive-statistics",
    "title": "Descriptive statistics",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nDefinition\nDescriptive statistics summarize and organize data to make it easier to understand, often using measures such as mean, median, variance, standard deviation, but also the number of observations.\n\n\nImplementation\nIn R, descriptive statistics can be implemented using built-in functions such as mean(), median(), sd(), and summary(), or through the dplyr package for more flexible and readable workflows. The key function is summarize() (or summarise(), R is relatively flexible between UK and US English).\nIf you want to summarize by group, make sure to group your data first using group_by().\n\n\nDepartmental population\nThe first question we want to answer is fairly simple: what is the population in each département?\n\n\n\n\n\n\nExercise\n\n\n\nFrom the idf_data data, create a new data frame with the population at the département level.\n\nCreate a variable with the dep ID (hint: use substr() to extract the first two digits of the municipal ID.\nGroup by département and compute the sum of population\n\n\n\n\n\nHousing price by city size\nNow, we want to test if city size correlates with price. A visual approach would be interesting (and we are actually going to do it later on), but so far, we stick with the numeric approach.\nHow can we proceed?\n\n\n\n\n\n\nExercise\n\n\n\nFrom the idf_data data, create a new data frame which provides the first/second/third quartile as well as the mean of housing cost by group of city sizes. You decide on the definition of the groups.",
    "crumbs": [
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "part2_desc.html#linear-regression",
    "href": "part2_desc.html#linear-regression",
    "title": "Descriptive statistics",
    "section": "Linear regression",
    "text": "Linear regression\nThe point of this class is not to teach you much about linear regression. The baseline model is:\n\\[\ny_i = \\alpha + \\beta x_i + e_i\n\\]\nThe obtained coefficient in this model gives the correlation between \\(x_i\\) and \\(y_i\\). Under specific conditions (notably, see the Gauss-Markov theorem), it is possible to draw causal conclusions on the impact of \\(x\\) on \\(y\\), but this is beyond the scope of this class.\nThe base function for linear regression is lm(y ~ x, data). However, I highlight a much more powerful and flexible package, either to regress but also to export results, fixest::feols(), which works with the same syntax and allows to implement so many more linear models and extensions.\n\n\n\n\n\n\nExercise\n\n\n\n\nWhat is the correlation coefficient between population and housing cost?\nCreate two new columns: log population and log housing cost\nRerun the model with the logged variables. Do the results make sense?",
    "crumbs": [
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "part2_desc.html#export-to-.tex",
    "href": "part2_desc.html#export-to-.tex",
    "title": "Descriptive statistics",
    "section": "Export to .tex",
    "text": "Export to .tex\n\n\n\n\n\n\nImportant\n\n\n\nIf you are not familiar with LaTeX, I more than warmly recommand you to learn it!! The learning curve is extremely steep, it shouldn’t take you more than a couple of hours to understand the base of it. The online community is superb and LLMs are extremely efficient at it. A good starting point is to use Overleaf and follow their guide to learn LaTeX in 30 minutes.\n\n\nSeveral libraries exist to export tables as .tex format (or in plain text, or in html). Among the most popular packages, we are going to use stargazer, which is super flexible and well-suited for descriptive statistics, knitr which is helpful to export data frames, and the etable function from fixest.\n\nExport the count of population\n\n\n      # Export the population \n      stargazer(\n        pop_dep,\n        summary = FALSE,\n        rownames = FALSE,\n        title = \"Population by department (2021)\",\n        label   = \"tab:pop\",\n        covariate.labels = c(\"Dep.\", \"Population\"),\n        notes = \"Source: INSEE, Recensement Permanent (2021).\",\n        out = paste0(path,\"/output/pop_dep.tex\")\n      )\n\nLet’s break down together the function!\n\nExport the rent data\n\n\n# Export the housing cost by city size\n      cost_size = cost_size %&gt;%\n        mutate(city_size = recode(city_size,\n                                  \"1_vsmall\" = \"Very Small\",\n                                  \"2_small\"  = \"Small\",\n                                  \"3_medium\" = \"Medium\",\n                                  \"4_large\"  = \"Large\",\n                                  \"5_vlarge\" = \"Very Large\"\n        )) %&gt;%\n        mutate(type = recode(type,\n                             \"appartement\" = \"Appart.\",\n                             \"house\"       = \"House\")) %&gt;%\n        mutate(across(where(is.numeric), ~as.numeric(round(.x, 2))))\n      \n      # Export with knitr \n      export = cost_size %&gt;%\n        mutate(city_size = factor(city_size, levels = c(\"Very Small\", \"Small\", \"Medium\", \"Large\", \"Very Large\"))) %&gt;%\n        arrange(city_size, type) %&gt;%\n        mutate(city_size = \"\") %&gt;%\n        select(type, mean, q1, q2, q3) %&gt;%\n        knitr::kable(\"latex\", booktabs = TRUE, longtable = TRUE,\n                     col.names = c(\"\\\\hspace{1em}\", \"Type\", \"Mean\", \"Q1\", \"Q2\", \"Q3\"),\n                     escape = FALSE) %&gt;%\n        # Add group rows for each city_size\n        kableExtra::group_rows(\"Very Small\", 1, 2, latex_gap_space = \"0.5em\") %&gt;%\n        kableExtra::group_rows(\"Small\", 3, 4, latex_gap_space = \"0.5em\") %&gt;%\n        kableExtra::group_rows(\"Medium\", 5, 6, latex_gap_space = \"0.5em\") %&gt;%\n        kableExtra::group_rows(\"Large\", 7, 8, latex_gap_space = \"0.5em\") %&gt;%\n        kableExtra::group_rows(\"Very Large\", 9, 10, latex_gap_space = \"0.5em\") %&gt;%\n        kableExtra::kable_styling(latex_options = c(\"hold_position\"))\n      \n      writeLines(export, paste0(path,\"/output/cost_size.tex\"))\n\nLet’s break down the function.\n\nExport the regressions\n\n\n      # Export the two regressions with etable\n      setFixest_dict(c(popmun = \"Population\", log_popmun = \"Population (log)\",\n                       loy_appartement = \"Rent\", log_loy_appartement = \"Rent (log)\" \n      ))\n      \n      \n      export = etable(reg, tex = T, style.tex = style.tex(\"aer\"), fitstat = ~ r2 + n)\n      writeLines(export, paste0(path,\"/output/reg.tex\"))\n      \n      export = etable(reg_log, tex = T, style.tex = style.tex(\"aer\"), fitstat = ~ r2 + n)\n      writeLines(export, paste0(path,\"/output/reg_log.tex\"))",
    "crumbs": [
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "part3_viz.html#standard-graphs",
    "href": "part3_viz.html#standard-graphs",
    "title": "Data visualisation",
    "section": "Standard graphs",
    "text": "Standard graphs\n\nScatter plot\nIn the previous section, we tried to understand the relationship between rents and population size at the municipal level. As we use bivariate analysis, a data visualisation is useful to provide intuition.\nTo make a very standard graph:\n\nggplot(idf_data, aes(x = loy_house, y = popmun)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis graph suffers from two issues: it is not visually appealing nor clear and it is not readable. We are going to take advantage of the power and high flexibility of the ggplot package to increase the quality of the graph. Moreover, we are going to use logged variables to make the data more visually appealing.\n\nggplot(idf_data, aes(x = log_loy_house, y = log_popmun)) +\n        geom_point()\n\n\n\n\n\n\n\n\nNow that graph is more readble, we can plot several columns at once. Here, the aes() for y is the same for all layers (two layers of points), so keep it in the main ggplot() options. FOr each layer, we specify for x but also for colour. Each group we defined is going to be a specific colour.\n\nggplot(idf_data, aes(y = log_popmun)) +\n  geom_point(aes(x = log_loy_appartement, colour = \"Appartement\")) +\n  geom_point(aes(x = log_loy_house, colour = \"House\"))\n\n\n\n\n\n\n\n\nR applies the standard color scheme. Colours can be chosen manually (we will see that after) or thanks to a scale from a package. The most popular package is viridis which provides a lot of pallettes for ggplot. It is especially useful as those palettes are suited for colourblind people (more than a tenth of the population!). Because we set the aes colour, we use scale_colour_viridis_d(). The d stands for discrete, as opposed to continuous.\n\nggplot(idf_data, aes(y = log_popmun)) +\n  geom_point(aes(x = log_loy_appartement, colour = \"Appartement\")) +\n  geom_point(aes(x = log_loy_house, colour = \"House\")) +\n  scale_color_viridis_d() # \n\n\n\n\n\n\n\n\nR offers a number of built-in “themes” to control the general appearance of the graph (backgroud, axes, minor grids, etc.). Here, I use theme_bw(). You can try: theme_minimal(), theme_classic(), theme_void(). Instead of using viridis, I set manually the colour for each group I defined.\n\nggplot(idf_data, aes(y = log_popmun)) +\n  geom_point(aes(x = log_loy_appartement, colour = \"Appartement\"), alpha = 0.5) +\n  geom_point(aes(x = log_loy_house, colour = \"House\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"House\" = \"red\", \"Appartement\" = \"navy\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\nI do not like the legend on the right of the graph, I want it inside the plot region. Here, it is important to set the theme() control after the theme_bw() because it overwrites it (as most command on R).\n\nggplot(idf_data, aes(y = log_popmun)) +\n  geom_point(aes(x = log_loy_appartement, colour = \"Appartement\"), alpha = 0.5) +\n  geom_point(aes(x = log_loy_house, colour = \"House\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"House\" = \"red\", \"Appartement\" = \"navy\")) +\n  theme_bw() +\n  theme(legend.position = c(0.15,0.9),\n        legend.title    = element_blank())\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nTo stick with the publication standard, I adapt the names of the axes and the title of the graph.\n\nggplot(idf_data, aes(y = log_popmun)) +\n  geom_point(aes(x = log_loy_appartement, colour = \"Appartement\"), alpha = 0.5) +\n  geom_point(aes(x = log_loy_house, colour = \"House\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"House\" = \"red\", \"Appartement\" = \"navy\")) +\n  theme_bw() +\n  theme(legend.position = c(0.15,0.9),\n        legend.title    = element_blank()) +\n  labs(x = \"Rent (log)\", y = \"Population (log)\", title = \"Correlation between rents and population\")\n\n\n\n\n\n\n\n\nOne additional graph, plotting the relationship we presented in the previous section. The layer for regression lines is geom_smooth. I also play with the location, the size, and the centeredness, of the title.\n\nggplot(idf_data, aes(y = log_popmun, x = log_loy_house)) +\n  geom_point(aes(x = log_loy_house), colour = \"red\", alpha = 0.5) +\n  geom_smooth(method = \"lm\", colour = \"black\") +\n  theme_classic() +\n  theme(panel.grid.major = element_line(colour = \"darkgray\", linetype = \"dashed\"),\n        panel.grid.minor = element_blank(),\n        plot.title=element_text(colour = \"black\", face = \"bold\", hjust = 0.5, size = 14)) +\n  labs(x = \"Rent (log)\", y = \"Population (log)\", title = \"Correlation between rents and population\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nBar charts\nIn the population data, we know the number of each CSP (socio-professional categories) in the population. Our first exercise will be to make a bar chart out of those data, with the average share of each CSP.\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing tidyr::pivot_wider(), create a long dataframe containing, per municipality, the count of each CSP.\nBy CSP, summarize the count of each CSP across all cities.\nCompute the share of each CSP.\n\nMy data looks like that:\n# A tibble: 6 × 4\n  csp      pop_csp      pop share_csp\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 agri       5600. 9654776.  0.000580\n2 autsap  1860539. 9654776.  0.193   \n3 cadre   1900614. 9654776.  0.197   \n4 employ  1576028. 9654776.  0.163   \n5 ouvri    824251. 9654776.  0.0854  \n6 profint 1581987. 9654776.  0.164  \n\nUsing geom_col(), make a barchart. To add colour, use fill = csp. Store the graph in an object. Export it as .pdf.\n\n\n\nBecause the csp column has information that are not straightfoward to interpret, I use the following commands to manually adapt the labels on the x axis. With theme it is possible to add a bit of angle to make it more readable.\n\np = p +\n  scale_x_discrete(labels = c(\"agri\"=\"Agriculture\", \n                                      \"cadre\"=\"Executives\", \n                                      \"profint\"=\"Intermediate\", \n                                      \"employ\"=\"Employees\", \n                                      \"ouvri\"=\"Workers\", \n                                      \"retraite\"=\"Retired\", \n                                      \"autsap\"=\"Other inactive\")) +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1)) \n\nUsing the function plotly::ggplotly(p, tooltip = \"text\"), make it interactive!\n\n\nBox plots\nFor each CSP, what is the distribution of the count across municipalities? Use the layer geom_boxplot() to know it!\n\n\nDensity plots\nAnother way to possibility to grasp on the distribution is to use a density plot:\n\nidf_data %&gt;%\n        tidyr::pivot_longer(cols = c(agri,cadre,profint,employ,ouvri,retraite,autsap),\n                            names_to = \"csp\",\n                            values_to = \"pop_csp\") %&gt;%\n        mutate(share_csp = pop_csp/popmun) %&gt;%\n        filter(csp %in% c(\"ouvri\",\"retraite\")) %&gt;%\n        ggplot(aes(x = share_csp, fill = csp)) +\n        geom_density(alpha = 0.5) +\n        theme_classic() +\n        theme(legend.position = c(0.8,0.7),\n              legend.title    = element_blank()) +\n        scale_fill_manual(values = c(\"ouvri\" = \"#440154\", \"retraite\" = \"#21908C\"),\n                          labels = c(\"ouvri\" = \"Blue collar\", \"retraite\" = \"Retired\")) +\n        labs(x = \"Share in population\", y = \"Density\") \n\n\n\n\n\n\n\nExercise\n\n\n\nThe density function is computed across all areas. What if we make a density plot per departement.\n\nAdd a column dep to the dataset (as the first two characters of codgeo)\nAdd a the command facet_wrap(~dep) to the previous density plot\nInterpret!",
    "crumbs": [
      "Data visualization"
    ]
  },
  {
    "objectID": "part4_spatial.html",
    "href": "part4_spatial.html",
    "title": "Data visualisation",
    "section": "",
    "text": "In this part, we are going to learn how to:\n\nUse georeferenced data\nCompute distances\nMake maps",
    "crumbs": [
      "Spatial data"
    ]
  },
  {
    "objectID": "part4_spatial.html#roadmap",
    "href": "part4_spatial.html#roadmap",
    "title": "Data visualisation",
    "section": "",
    "text": "In this part, we are going to learn how to:\n\nUse georeferenced data\nCompute distances\nMake maps",
    "crumbs": [
      "Spatial data"
    ]
  },
  {
    "objectID": "part4_spatial.html#a-primer-on-spatial-data",
    "href": "part4_spatial.html#a-primer-on-spatial-data",
    "title": "Data visualisation",
    "section": "A primer on spatial data",
    "text": "A primer on spatial data\nWe are going to take advantage of the/temp/idf_data.csv we built earlier on, monu_histo.csv containing the list of all monuments listed in Ile-de-France with the zip code, gares.shp listing all stations in the Île-de-France train network, with their associated location.\n\nShapefiles\nSpatial data can come in different format but the most popular one is the shapefile .shp. As the name suggested, it is a file containing shapes. Most common shapes are: POINTS, LINES, POLYGONS. As the .csv format, each line contains a shape and can contain other information. Spatial data are usually spatially projected. The most common projection in Western countries is the Mercator projection. In Europe, the projection is usually WGS84. In France, the projection is Lambert 93. Projection also contains information on the units (meters or degrees or kilometers), the position with respect to the meridians, etc.\nShapefiles are delivered in folders containing at least 3 files: .dbf (with the data), .shx (indexing the geometry to the data), .shp (containing the shapes). Usually, we find also a .prj (the projection of the data). You need to have all files within the same folder to be able to load the .shp with any software (such as QGIS, Stata, R, ArcGIS, or others).\n\n\nSpatial data in R\nSpatial data can be opened in R thanks to sf package. To open a shapefile, the function is sf::read_sf(). It creates a specific object type, an sf object in R. All objects of that type contain a column geometry by default. To transform an sf object into a data.frame again, the good practice is df = df_sf %&gt;% st_drop_geometry() %&gt;% as.data.frame().\nMost functions (plots, summarize, joint) are compatible with sf objects. However, due to their spatial features, we can apply many other (similar) functions.",
    "crumbs": [
      "Spatial data"
    ]
  },
  {
    "objectID": "part3_viz.html#adding",
    "href": "part3_viz.html#adding",
    "title": "Data visualisation",
    "section": "Adding",
    "text": "Adding",
    "crumbs": [
      "Data visualization"
    ]
  },
  {
    "objectID": "part4_spatial.html#price-gradient",
    "href": "part4_spatial.html#price-gradient",
    "title": "Data visualisation",
    "section": "Price gradient",
    "text": "Price gradient\nThe objective is to descriptively assess a relationship (although non-causal) between rents and the distance to the Eiffel Tower. Let’s do it step-by-step:\n\nOpen the shapefiles of cities\n\n\nco_shp = read_sf(paste0(path,\"/raw_data/communes.shp\")) %&gt;%\n        select(insee, geometry, nomcom) %&gt;%\n        rename(codgeo = insee) # Prepare the match for the further use\n\n\nOpen the monument data and make it a shapefile\n\n\neiffel = read.csv2(paste0(path,\"/raw_data/monu_histo.csv\")) %&gt;%\n        filter(eiffel == 1) %&gt;%\n        st_as_sf(coords = c(\"y_wgs84\",\"x_wgs84\"), crs = st_crs(co_shp))\n\nHere, notice that we set the crs (the projection) of the shape so that it matches the one from cities.\n\nCompute the distance between each city centroid and the Eiffel Tower\n\n\nco_eiffel = co_shp %&gt;%\n        mutate(centroid = st_centroid(geometry)) %&gt;%\n        mutate(distance = as.numeric(st_distance(centroid,eiffel$geometry[1]))/1000) %&gt;%\n        select(-centroid) \n\n\nPlot the price gradient\n\n\n\nJoining with `by = join_by(codgeo)`\n`geom_smooth()` using method = 'gam'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (if time allows)\n\n\n\nDo it yourself!",
    "crumbs": [
      "Spatial data"
    ]
  },
  {
    "objectID": "part4_spatial.html#maps",
    "href": "part4_spatial.html#maps",
    "title": "Data visualisation",
    "section": "Maps",
    "text": "Maps\n\nTrain stations in Ile-de-France\nThe point is to plot the average distance to the nearest train station for each city in Ile-de-France.\n\n      stations = read_sf(paste0(path,\"/raw_data/gares.shp\")) %&gt;%\n        select(mode,exploitant,geometry)\n      \n      # Visualize the stations\n      ggplot() +\n        geom_sf(data = co_shp) +\n        geom_sf(data = stations, size = 0.4) +\n        theme_void()\n\n      # Remove the stations out of IDF\n      idf_stations = stations %&gt;%\n        st_join(co_shp, join = st_within, left = FALSE)\n      \n      # Visualize the stations\n      ggplot() +\n        geom_sf(data = co_shp) +\n        geom_sf(data = idf_stations, size = 0.4) +\n        theme_void()\n      \n      # Keep only those with a RER \n      idf_stations_rer = idf_stations %&gt;%\n        filter(grepl(\"RER\", mode))\n\n      # Distance between the centroids and the nearest RER station + map\n      co_shp %&gt;%\n        mutate(centroid = st_centroid(geometry)) %&gt;%\n        mutate(distance = as.numeric(st_distance(centroid, idf_stations_rer) %&gt;% apply(1, min))/1000) %&gt;%\n        select(-centroid) %&gt;%\n        ggplot() +\n        geom_sf(aes(fill = distance), colour = NA) +\n        scale_fill_viridis_c(option = \"B\", name = NULL) +\n        theme_void() +\n        labs(title = \"Distance (km) to the nearest station\") +\n        theme(plot.title = element_text(colour = \"black\", face = \"bold\", hjust = 0.5, size = 14))\n\n\n\n\n\n\n\nExercise\n\n\n\nFollowing the work on the train stations, we would like to make two maps: one with the total number of stops per city (outside of Paris but still plotting Paris!!) and one with the total number of RER stops per city.\n\nOpen the stations and municipal shapefiles\nKeep only stations within Ile de France and outside Paris\nSummarize by municipality the count of stations (total and only RER)\nMake the maps using geom_sf()\nAdd a label to tag the city the most stations\n\n\n\n\n\nMerging geometries\nOne last element I believe is important is the spatial manipulation of geometries. sf is an extremely powerful package to manipulate geometries altogether. If you use spatial data, you may want for instance to crop geometries to keep the intersection (then st_intersection is your friend). Here, we use st_union() to merge geometries together.\nBecause we have municipalities, we may want to plot based on département instead. Hence, we use summarize(geometry = sf::st_union(geometry)) to merge geometries by group:\n\ndep_shp = co_shp %&gt;%\n        group_by(dep = substr(codgeo,1,2)) %&gt;%\n        summarize(geometry = st_union(geometry))\n\n\n\n\n\n\n\nExercise\n\n\n\nPlot the number of monuments per département",
    "crumbs": [
      "Spatial data"
    ]
  }
]